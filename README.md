# üéØ Projeto BigQuery

Projeto de realiza√ß√£o de PoC de cria√ß√£o de processo de configura√ß√£o, ingest√£o e consumo de uma fonte de dados no BigQuery.

# üìä Documenta√ß√£o do Pipeline de Dados: Ingest√£o no GCP e Visualiza√ß√£o com Power BI

Este reposit√≥rio descreve a **Fase 1 de um pipeline de dados completo**, focado na ingest√£o e disponibiliza√ß√£o de dados no Google Cloud Platform (GCP), com posterior uso no Power BI para visualiza√ß√£o.

---

## üìå Fase 1: Obten√ß√£o e Disponibiliza√ß√£o dos Dados (API + Cloud Storage)

Nesta etapa, o objetivo √©:

* Obter os dados de um arquivo CSV local
* Criar uma API com **FastAPI** que exp√µe esses dados no endpoint `/veiculos`
* Realizar o upload do arquivo para o **Cloud Storage (GCP)**
* Automatizar o processo de ingest√£o com **Cloud Run + Cloud Functions**

> ‚ö†Ô∏è **Pr√©-requisitos**:
>
> * Realize as configura√ß√µes de ambiente e permiss√µes descritas abaixo.
> * Mantenha os arquivos organizados por fun√ß√£o (API e fun√ß√£o de carga).
> * Utilize a ferramenta de debug do VS Code para valida√ß√µes locais.

---

## ‚òÅÔ∏è Provisionamento no Google Cloud Platform (GCP)

### 1. Criar uma conta gratuita

Acesse: [https://cloud.google.com](https://cloud.google.com)
‚Üí A conta gratuita oferece **US\$ 300 em cr√©ditos por 90 dias**

### 2. Criar um projeto

Nome sugerido: `bigquery-ingestion`

### 3. Criar um bucket no Cloud Storage

* Acesse: [Console do GCP ‚Äì Storage](https://console.cloud.google.com/storage)
* Nome do bucket: `meu-bucket-dados-poc`

### 4. Criar uma conta de servi√ßo com chave JSON

* Acesse: [Contas de Servi√ßo ‚Äì IAM](https://console.cloud.google.com/iam-admin/serviceaccounts)
* Nome: `servicos-poc`
* Permiss√£o: `Storage Object Admin` (`roles/storage.objectAdmin`)
* Gere e **baixe a chave JSON**
* Mova o arquivo para a pasta do projeto

---

## ‚öôÔ∏è Configura√ß√£o do Ambiente de Desenvolvimento

### 1. Instalar Python 3.10+ no macOS

‚û°Ô∏è [Download Python para macOS](https://www.python.org/downloads/mac-osx/)

### 2. Instalar Visual Studio Code

‚û°Ô∏è [Download VS Code](https://code.visualstudio.com/)
Extens√µes recomendadas: **Python**, **Pylance**

### 3. Instalar depend√™ncias

```bash
pip install fastapi uvicorn pandas google-cloud-storage requests
```

---

## üìÑ Scripts da Fase 1

### üîπ API com FastAPI ‚Äî `main.py`

```python
from fastapi import FastAPI
import pandas as pd
import os

app = FastAPI()
CSV_PATH = "veiculos_mais_vendidos_2024_completo.csv"

@app.get("/")
def home():
    return {"mensagem": "API est√° rodando. Acesse /veiculos para ver os dados."}

@app.get("/veiculos")
def listar_veiculos():
    if not os.path.exists(CSV_PATH):
        return {"erro": "Arquivo CSV n√£o encontrado."}
    df = pd.read_csv(CSV_PATH)
    return df.to_dict(orient="records")
```

### ‚ñ∂Ô∏è Executar a API no Terminal Bash

```bash
uvicorn main:app --reload
```

---

## üöÄ Publica√ß√£o da API no Cloud Run

### Estrutura b√°sica do reposit√≥rio:

```
üìÅ api/
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ veiculos_mais_vendidos_2024_completo.csv
```

### Passo a passo realizado:

1. Acessar o Console do GCP > Cloud Run > Criar Servi√ßo
2. Selecionar a origem do c√≥digo como GitHub (op√ß√£o de CI/CD)
3. Escolher a branch `main` e tipo de build `buildpacks`
4. Informar a linguagem: Python 3.10
5. Deixar o ponto de entrada em branco (FastAPI detecta automaticamente)
6. Habilitar acesso n√£o autenticado
7. Implantar o servi√ßo e copiar o endpoint gerado

---

## üîÅ Script de Ingest√£o Autom√°tica via Cloud Run (Editor In-line)

### üîπ C√≥digo da fun√ß√£o ‚Äî `main.py`

```python
import functions_framework
import pandas as pd
import requests
from google.cloud import storage

NOME_BUCKET = "meu-bucket-dados-poc"
DESTINO_NO_BUCKET = "documento_poc/veiculos_mais_vendidos_2024_completo.csv"
URL_API_VEICULOS = "https://sua-api-url.a.run.app/veiculos"  # substitua pela URL real

@functions_framework.http
def carga_csv_para_gcs(request):
    try:
        response = requests.get(URL_API_VEICULOS)
        response.raise_for_status()
        dados = response.json()

        df = pd.DataFrame(dados)
        caminho_tmp = "/tmp/veiculos_mais_vendidos.csv"
        df.to_csv(caminho_tmp, index=False)

        client = storage.Client()
        bucket = client.bucket(NOME_BUCKET)
        blob = bucket.blob(DESTINO_NO_BUCKET)
        blob.upload_from_filename(caminho_tmp)

        return {"status": "sucesso", "mensagem": f"Arquivo enviado para gs://{NOME_BUCKET}/{DESTINO_NO_BUCKET}"}

    except Exception as e:
        return {"status": "erro", "mensagem": str(e)}
```

### üîπ `requirements.txt`

```
functions-framework
pandas
requests
google-cloud-storage
```

### Passo a passo realizado para o script de carga:

1. Acessar Cloud Run > Criar servi√ßo > op√ß√£o ‚ÄúEscrever fun√ß√£o‚Äù
2. Inserir o nome `carga-poc`, selecionar a regi√£o `southamerica-east1`
3. Escolher linguagem Python 3.10
4. Inserir os arquivos `main.py` e `requirements.txt` manualmente
5. No campo ‚ÄúPonto de entrada da fun√ß√£o‚Äù, digitar `carga_csv_para_gcs`
6. Habilitar acesso n√£o autenticado
7. Implantar o servi√ßo
8. Testar a fun√ß√£o acessando o endpoint via navegador ou curl

---

## ‚úÖ Status da Fase 1

* ‚úÖ API funcional com dados do CSV publicada no Cloud Run
* ‚úÖ Upload automatizado com fun√ß√£o em Cloud Run
* ‚úÖ Dados dispon√≠veis no Cloud Storage

---

‚û°Ô∏è **Com o arquivo `veiculos_mais_vendidos_2024_completo.csv` armazenado automaticamente no bucket `meu-bucket-dados-poc`, a Fase 1 est√° conclu√≠da.**

Pr√≥xima etapa: **ingest√£o no BigQuery e visualiza√ß√£o no Power BI.**
