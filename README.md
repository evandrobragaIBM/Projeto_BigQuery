# üéØ Projeto BigQuery
Projeto de realiza√ß√£o de PoC de cria√ß√£o de proceso de configura√ß√£o, ingest√£o e consumo de uma fonte de dados no BigQuery.

# üìä Documenta√ß√£o do Pipeline de Dados: Ingest√£o no GCP e Visualiza√ß√£o com Power BI

Este reposit√≥rio descreve a **Fase 1 de um pipeline de dados completo**, focado na ingest√£o e disponibiliza√ß√£o de dados no Google Cloud Platform (GCP), com posterior uso no Power BI para visualiza√ß√£o.

---

## üìå Fase 1: Obten√ß√£o e Disponibiliza√ß√£o dos Dados (API + Cloud Storage)

Nesta etapa, o objetivo √©:

- Obter os dados de um arquivo CSV local
- Criar uma API com **FastAPI** que exp√µe esses dados no endpoint `/veiculos`
- Realizar o upload do arquivo para o **Cloud Storage (GCP)** para posterior integra√ß√£o com BigQuery

> ‚ö†Ô∏è **Pr√©-requisitos**:
> - Realize as configura√ß√µes de ambiente e permiss√µes descritas abaixo.
> - Mantenha os arquivos na mesma pasta do projeto para melhor funcionamento.
> - Utilize a ferramenta de debug do VS Code para valida√ß√µes locais.

---

## ‚òÅÔ∏è Provisionamento no Google Cloud Platform (GCP)

### 1. Criar uma conta gratuita
Acesse: [https://cloud.google.com](https://cloud.google.com)  
‚Üí A conta gratuita oferece **US$ 300 em cr√©ditos por 90 dias**

### 2. Criar um projeto
Nome sugerido: `bigquery-ingestion`

### 3. Criar um bucket no Cloud Storage
- Acesse: [Console do GCP ‚Äì Storage](https://console.cloud.google.com/storage)
- Nome do bucket: `seu-bucket`

### 4. Criar uma conta de servi√ßo com chave JSON
- Acesse: [Contas de Servi√ßo ‚Äì IAM](https://console.cloud.google.com/iam-admin/serviceaccounts)
- Nome: `servicos-poc`
- Permiss√£o: `Storage Object Admin` (`roles/storage.objectAdmin`)
- Gere e **baixe a chave JSON**
- Mova o arquivo para a pasta do projeto

---

## ‚öôÔ∏è Configura√ß√£o do Ambiente de Desenvolvimento

### 1. Instalar Python 3.10+ no macOS
‚û°Ô∏è [Download Python para macOS](https://www.python.org/downloads/mac-osx/)

### 2. Instalar Visual Studio Code
‚û°Ô∏è [Download VS Code](https://code.visualstudio.com/)  
Extens√µes recomendadas: **Python**, **Pylance**

### 3. Instalar depend√™ncias

```bash
pip install fastapi uvicorn pandas google-cloud-storage
```

### IMPORTANTE: pip3 para Mac e sempre utilizar Bash.

---

## üìÑ Scripts da Fase 1

### üîπ API com FastAPI ‚Äî `main.py`

```python
from fastapi import FastAPI
import pandas as pd
import os

app = FastAPI()
CSV_PATH = "veiculos_mais_vendidos_2024_completo.csv"

@app.get("/")
def home():
    return {"mensagem": "API est√° rodando. Acesse /veiculos para ver os dados."}

@app.get("/veiculos")
def listar_veiculos():
    if not os.path.exists(CSV_PATH):
        return {"erro": "Arquivo CSV n√£o encontrado."}
    df = pd.read_csv(CSV_PATH)
    return df.to_dict(orient="records")
```

### ‚ñ∂Ô∏è Executar a API no Terminal Bash

```bash
uvicorn main:app --reload
```

---

### üîπ Script de upload para o GCS ‚Äî `ingestao_dados_gcs.py`

```python
from google.cloud import storage

# Caminho do arquivo CSV local
CAMINHO_LOCAL = "/Users/seu-user/Desktop/Projeto Ingestion_Query/veiculos_mais_vendidos_2024_completo.csv"

# Nome do bucket j√° criado no seu projeto
NOME_BUCKET = "seu-bucket" # Adicione o nome do seu bucket criado
# Caminho no bucket onde o arquivo ser√° armazenado
DESTINO_NO_BUCKET = "Documento PoC/veiculos_mais_vendidos_2024_completo.csv"

def upload_csv_para_bucket():
    # Cria cliente autenticado
    client = storage.Client()
    
    # Refer√™ncia ao bucket
    bucket = client.bucket(NOME_BUCKET)
    
    # Cria refer√™ncia ao arquivo (blob) de destino
    blob = bucket.blob(DESTINO_NO_BUCKET)
    
    # Faz o upload
    blob.upload_from_filename(CAMINHO_LOCAL)
    
    print(f"‚úîÔ∏è Upload conclu√≠do: gs://{NOME_BUCKET}/{DESTINO_NO_BUCKET}")

if __name__ == "__main__":
    upload_csv_para_bucket()
```

> ‚ö†Ô∏è Antes de rodar esse script, defina a vari√°vel de ambiente. Rode o comando junto no terminal:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/Users/seu-user/Desktop/Projeto Ingestion_Query/nome-da-sua-chave.json"
python ingestao_dados_gcs.py
```

---

### üöß Poss√≠veis erros e solu√ß√£o

#### Erro:
```
Permission 'storage.objects.create' denied
```

#### Solu√ß√£o:
1. Acesse o bucket no console GCP
2. V√° at√© a aba **"Permiss√µes"**
3. Clique em **"Conceder acesso"**
4. Informe o **e-mail da conta de servi√ßo** (dispon√≠vel no JSON)
5. Atribua a permiss√£o: `Storage Object Admin`
6. Salve e tente novamente

---

## ‚úÖ Status da Fase 1

- ‚úÖ API funcional com dados do CSV
- ‚úÖ Upload para o Cloud Storage realizado com sucesso
- ‚úÖ Autentica√ß√£o via conta de servi√ßo configurada

---

‚û°Ô∏è **Com o arquivo `veiculos_mais_vendidos_2024_completo.csv` armazenado no bucket `seu-bucket`, a Fase 1 est√° conclu√≠da.**

Pr√≥xima etapa: **ingest√£o automatizada no BigQuery e visualiza√ß√£o no Power BI.**
